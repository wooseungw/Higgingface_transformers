{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\transformer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.35k/1.35k [00:00<?, ?B/s]\n",
      "Downloading model.safetensors: 100%|██████████| 5.31G/5.31G [00:54<00:00, 96.7MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 3.91MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.33MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 200/200 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   40, 16726,   262,  2854,   286,   402, 11571,    12,  8199,    78,\n",
      "         4166,   416,  4946, 20185,    13])\n",
      "I evaluated the performance of GPT-Neo developed by OpenAI.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징을 통한 인코딩(입력문이 하나인 경우)\n",
    "input = tokenizer.encode(\"I evaluated the performance of GPT-Neo developed by OpenAI.\", return_tensors=\"pt\")\n",
    "print(input[0])\n",
    "print(tokenizer.decode(input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40, 16726,   262,  2854,   286,   402, 11571,    12,  8199,    78,\n",
      "          4166,   416,  4946, 20185,    13],\n",
      "        [   40, 16726,   262,  2854,   286,   402, 11571,  4166,   416,  4946,\n",
      "         20185,    13, 50257, 50257, 50257]])\n",
      "['I evaluated the performance of GPT-Neo developed by OpenAI.', 'I evaluated the performance of GPT developed by OpenAI. [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징을 통한 인코딩(입력문이 복수인 경우)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "input = tokenizer.batch_encode_plus([\"I evaluated the performance of GPT-Neo developed by OpenAI.\",\"I evaluated the performance of GPT developed by OpenAI.\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(input['input_ids'])\n",
    "print([tokenizer.decode(input['input_ids'][i]) for i in range(len(input['input_ids']))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1\n",
      "I evaluated the performance of the proposed method on the real-world dataset. The results are shown in\n",
      "\n",
      "No.2\n",
      "Vaccine for new-borns\n",
      "\n",
      "The vaccine for new-borns is a vaccine\n",
      "\n",
      "No.3\n",
      "3.1415926535897932384626433832795028841971693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징\n",
    "input = tokenizer.batch_encode_plus([\"I evaluated the performance of GPT2 developed by OpenAI.\", \n",
    "                                     \"Vaccine for new coronavirus in the UK\",\n",
    "                                     \"3.1415926535\"], max_length=5, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "input['input_ids']\n",
    "generated = model.generate(input['input_ids'])\n",
    "generated_text = tokenizer.batch_decode(generated)\n",
    "\n",
    "for i, sentence in enumerate(generated_text):\n",
    "  print(f'No.{i+1}')\n",
    "  print(f\"{sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good thing to have\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\")\n",
    "model.to(device)\n",
    "\n",
    "# 토크나이징. 출력은 파이토치 텐서(pt)로 받음\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids.to(device), max_length=12).cpu()\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====출력====\n",
      "I like gpt because it's a good way to get\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model.to(device)\n",
    "\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "output = model.generate(input_ids.to(device), max_length=12).cpu()\n",
    "print(\"====출력====\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "[{'score': 0.25727882981300354, 'token': 2364, 'token_str': 'main', 'sequence': 'mlm and nsp is the main task of bert.'}, {'score': 0.2074068784713745, 'token': 3078, 'token_str': 'primary', 'sequence': 'mlm and nsp is the primary task of bert.'}, {'score': 0.06773312389850616, 'token': 2034, 'token_str': 'first', 'sequence': 'mlm and nsp is the first task of bert.'}, {'score': 0.06548517942428589, 'token': 2430, 'token_str': 'central', 'sequence': 'mlm and nsp is the central task of bert.'}, {'score': 0.06167394295334816, 'token': 3937, 'token_str': 'basic', 'sequence': 'mlm and nsp is the basic task of bert.'}]\n",
      "====================================================================================================\n",
      "[{'score': 0.2590245306491852, 'token': 3078, 'token_str': 'primary', 'sequence': 'mlm and nsp is the primary task of bert.'}, {'score': 0.1630989909172058, 'token': 2364, 'token_str': 'main', 'sequence': 'mlm and nsp is the main task of bert.'}, {'score': 0.081827811896801, 'token': 4563, 'token_str': 'core', 'sequence': 'mlm and nsp is the core task of bert.'}, {'score': 0.040237799286842346, 'token': 7037, 'token_str': 'dual', 'sequence': 'mlm and nsp is the dual task of bert.'}, {'score': 0.024844961240887642, 'token': 4054, 'token_str': 'principal', 'sequence': 'mlm and nsp is the principal task of bert.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "print(\"=\"*100)\n",
    "print(unmasker(\"MLM and NSP is the [MASK] task of BERT.\"))\n",
    "print(\"=\"*100)\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "print(unmasker(\"MLM and NSP is the [MASK] task of BERT.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 684/684 [00:00<?, ?B/s] \n",
      "Downloading model.safetensors: 100%|██████████| 47.4M/47.4M [00:03<00:00, 13.1MB/s]\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.weight', 'albert.pooler.bias']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 760k/760k [00:00<00:00, 21.1MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.31M/1.31M [00:00<00:00, 6.60MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.047601066529750824,\n",
       "  'token': 6612,\n",
       "  'token_str': 'ultimate',\n",
       "  'sequence': 'mlm and nsp is the ultimate task of bert.'},\n",
       " {'score': 0.024472149088978767,\n",
       "  'token': 20766,\n",
       "  'token_str': 'hardest',\n",
       "  'sequence': 'mlm and nsp is the hardest task of bert.'},\n",
       " {'score': 0.023495100438594818,\n",
       "  'token': 1256,\n",
       "  'token_str': 'primary',\n",
       "  'sequence': 'mlm and nsp is the primary task of bert.'},\n",
       " {'score': 0.02157510444521904,\n",
       "  'token': 407,\n",
       "  'token_str': 'main',\n",
       "  'sequence': 'mlm and nsp is the main task of bert.'},\n",
       " {'score': 0.018088050186634064,\n",
       "  'token': 18369,\n",
       "  'token_str': 'foremost',\n",
       "  'sequence': 'mlm and nsp is the foremost task of bert.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델명이 바뀌었음에 유의\n",
    "unmasker = pipeline('fill-mask', model='albert-base-v2')\n",
    "unmasker(\"mlm and nsp is the [MASK] task of bert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
