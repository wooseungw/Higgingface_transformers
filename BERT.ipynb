{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43-65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",num_labels = 3 )\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 감성 레이블을 숫자로 매칭하는 딕셔너리 준비\n",
    "dic = {0:'positive', 1:'neutral', 2:'negative'}\n",
    "\n",
    "# 입력 문장 데이터\n",
    "eval_list = [\"I like apple\", \"I like pear\", \"I go to school\", \"I dislike mosquito\", \"I felt very sad\", \"I feel so good\"]\n",
    "\n",
    "# 정답 레이블(answer label)\n",
    "ans = torch.tensor([0, 0, 1, 2, 2, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:I like apple\n",
      "positive:I like pear\n",
      "neutral:I go to school\n",
      "negative:I dislike mosquito\n",
      "negative:I felt very sad\n",
      "positive:I feel so good\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for article in eval_list:\n",
    "        inputs = tokenizer.encode(article, return_tensors=\"pt\",padding=True,truncation=True)\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        print(f\"{dic[logits.argmax(-1).item()]}:{article}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, loss:1.005324363708496\n",
      "epoch:2, loss:1.0017681121826172\n",
      "epoch:3, loss:0.9301735758781433\n",
      "epoch:4, loss:1.0353354215621948\n",
      "epoch:5, loss:1.0601882934570312\n",
      "epoch:6, loss:1.0591109991073608\n",
      "epoch:7, loss:0.9373170733451843\n",
      "epoch:8, loss:0.885542631149292\n",
      "epoch:9, loss:0.9669885635375977\n",
      "epoch:10, loss:0.8057706952095032\n",
      "epoch:11, loss:0.8437663912773132\n",
      "epoch:12, loss:0.8559024333953857\n",
      "epoch:13, loss:0.8128001689910889\n",
      "epoch:14, loss:0.8020715117454529\n",
      "epoch:15, loss:0.7041707038879395\n",
      "epoch:16, loss:0.6522318124771118\n",
      "epoch:17, loss:0.6728578209877014\n",
      "epoch:18, loss:0.6325636506080627\n",
      "epoch:19, loss:0.5774747729301453\n",
      "epoch:20, loss:0.6754273772239685\n",
      "epoch:21, loss:0.5964720845222473\n",
      "epoch:22, loss:0.4475818872451782\n",
      "epoch:23, loss:0.563477098941803\n",
      "epoch:24, loss:0.4801467955112457\n",
      "epoch:25, loss:0.5579213500022888\n",
      "epoch:26, loss:0.522051215171814\n",
      "epoch:27, loss:0.4860715866088867\n",
      "epoch:28, loss:0.44378066062927246\n",
      "epoch:29, loss:0.43595072627067566\n",
      "epoch:30, loss:0.4037271738052368\n",
      "epoch:31, loss:0.41637858748435974\n",
      "epoch:32, loss:0.3531412184238434\n",
      "epoch:33, loss:0.3545341491699219\n",
      "epoch:34, loss:0.36622533202171326\n",
      "epoch:35, loss:0.3112161457538605\n",
      "epoch:36, loss:0.3464055061340332\n",
      "epoch:37, loss:0.32895901799201965\n",
      "epoch:38, loss:0.29274460673332214\n",
      "epoch:39, loss:0.3634888231754303\n",
      "epoch:40, loss:0.24459415674209595\n",
      "epoch:41, loss:0.2514473497867584\n",
      "epoch:42, loss:0.27672818303108215\n",
      "epoch:43, loss:0.25434643030166626\n",
      "epoch:44, loss:0.30236032605171204\n",
      "epoch:45, loss:0.24200265109539032\n",
      "epoch:46, loss:0.2581678330898285\n",
      "epoch:47, loss:0.23793195188045502\n",
      "epoch:48, loss:0.19672633707523346\n",
      "epoch:49, loss:0.21913151443004608\n",
      "epoch:50, loss:0.26865580677986145\n"
     ]
    }
   ],
   "source": [
    "# 런타임 1분 소요\n",
    "# 활성화 함수 AdamW 불러오기\n",
    "from transformers import AdamW\n",
    "\n",
    "# 활성화 함수 AdamW 인스턴스화\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# 모델을 학습 모드로 변경\n",
    "model.train()\n",
    "\n",
    "# 에포크 수 지정 및 손실을 담은 빈 컨테이너 리스트 생성\n",
    "epochs = 50\n",
    "losses = []\n",
    "\n",
    "# 파인튜닝\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 그래디언트(기울기) 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 변수 eval_list에 담긴 여섯 개 문장을 토크나이저에 넣고 인코딩\n",
    "    inputs = tokenizer.batch_encode_plus(eval_list, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # 위에서 생성된 input 변수에 담긴 키(key)와 키값(value)을 **inputs 형식(**kwargs 형식)으로 모델에 전달\n",
    "    # 거기에 추가로 labels를 텐서 타입으로 모델에 전달\n",
    "    outputs = model(**inputs, labels=ans)\n",
    "\n",
    "    # 로짓 추출\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # 손실 추출\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # 오차역전파\n",
    "    loss.backward()\n",
    "\n",
    "    # 가중치(weight) 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    # 손실을 빈 컨테이너 losses에 순서대로 저장\n",
    "    losses.append(loss)\n",
    "\n",
    "    # 에포크 및 손실 값 출력\n",
    "    # 에포크는 0부터 시작하기에, 1을 더해줘서 사람들이 에포크 회수를 더 자연스럽게 인지하게 조치\n",
    "    print(f\"epoch:{epoch+1}, loss:{loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs# 파이토치에서 item( )은 GPU로부터 값을 추출하여 CPU로 전달\n",
    "new_losses = [i.item() for i in losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(new_losses);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: I like apple\n",
      "positive: I like pear\n",
      "neutral: I go to school\n",
      "negative: I dislike mosquito\n",
      "negative: I felt very sad\n",
      "positive: I feel so good\n"
     ]
    }
   ],
   "source": [
    "# 파인 튜닝 이후 추론\n",
    "dic = {0:'positive', 1:'neutral', 2:'negative'}\n",
    "eval_list = [\"I like apple\", \"I like pear\", \"I go to school\", \"I dislike mosquito\", \"I felt very sad\", \"I feel so good\"]\n",
    "\n",
    "# 모델을 eval 모델로 전환\n",
    "model.eval()\n",
    "\n",
    "# 모델 예측을 담을 preds라는 빈 컨테이너 리스트 생성\n",
    "preds = []\n",
    "\n",
    "# 이하 코드의 설명은 문제 45 코드 참조\n",
    "with torch.no_grad():\n",
    "  for article in eval_list:\n",
    "    inputs = tokenizer.encode(article, return_tensors=\"pt\",padding=True, truncation=True)\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits\n",
    "    pred = logits.argmax(-1).item()\n",
    "    preds.append(logits.argmax(-1).item())\n",
    "    print(f\"{dic[pred]}: {article}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 2, 2, 0])\n",
      "Accuracy:100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\우승우\\AppData\\Local\\Temp\\ipykernel_14180\\2938332119.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds = torch.tensor(preds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# item()에 의해 넘파이 숫자값이 된 preds를 다시 토치 텐서 타입으로 변환\n",
    "import torch\n",
    "preds = torch.tensor(preds)\n",
    "\n",
    "# preds 출력\n",
    "print(preds)\n",
    "\n",
    "# 정답(ans)와 예측(preds)가 일치하는 건수를\n",
    "# 정답(ans)의 전체 건수로 나누어서 정확도(Accuracy) 계산\n",
    "print(f\"Accuracy:{100 * sum(ans.detach().clone()==preds)/len(ans.detach().clone())}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.63k/1.63k [00:00<?, ?B/s]\n",
      "pytorch_model.bin: 100%|██████████| 1.02G/1.02G [00:10<00:00, 95.6MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 26.0kB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.13MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 768kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.71MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "\n",
      "Google LLC is an American multinational technology company focusing on search engine technology, online advertising, cloud computing, computer software, quantum computing, e-commerce, artificial intelligence,[9] and consumer electronics. It has been referred to as \"the most powerful company in the world\"[10] and one of the world's most valuable brands due to its market dominance, data collection, and technological advantages in the area of artificial intelligence.[11][12][13] Its parent company Alphabet is considered one of the Big Five American information technology companies, alongside Amazon, Apple, Meta, and Microsoft.\n",
      "\n",
      "Google was founded on September 4, 1998, by Larry Page and Sergey Brin while they were PhD students at Stanford University in California. Together they own about 14% of its publicly listed shares and control 56% of the stockholder voting power through super-voting stock. The company went public via an initial public offering (IPO) in 2004. In 2015, Google was reorganized as a wholly owned subsidiary of Alphabet Inc. Google is Alphabet's largest subsidiary and is a holding company for Alphabet's Internet properties and interests. Sundar Pichai was appointed CEO of Google on October 24, 2015, replacing Larry Page, who became the CEO of Alphabet. On December 3, 2019, Pichai also became the CEO of Alphabet.[14]\n",
      "\n",
      "The company has since rapidly grown to offer a multitude of products and services beyond Google Search, many of which hold dominant market positions. These products address a wide range of use cases, including email (Gmail), navigation (Waze & Maps), cloud computing (Cloud), web browsing (Chrome), video sharing (YouTube), productivity (Workspace), operating systems (Android), cloud storage (Drive), language translation (Translate), photo storage (Photos), video calling (Meet), smart home (Nest), smartphones (Pixel), wearable technology (Pixel Watch & Fitbit), music streaming (YouTube Music), video on demand (YouTube TV), artificial intelligence (Google Assistant), machine learning APIs (TensorFlow), AI chips (TPU), and more. Discontinued Google products include gaming (Stadia), Glass,[citation needed] Google+, Reader, Play Music, Nexus, Hangouts, and Inbox by Gmail.[15][16]\n",
      "\n",
      "Google's other ventures outside of Internet services and consumer electronics include quantum computing (Sycamore), self-driving cars (Waymo, formerly the Google Self-Driving Car Project), smart cities (Sidewalk Labs), and transformer models (Google Brain).[17]\n",
      "\n",
      "Google and YouTube are the two most visited websites worldwide followed by Facebook and Twitter. Google is also the largest search engine, mapping and navigation application, email provider, office suite, video sharing platform, photo and cloud storage provider, mobile operating system, web browser, ML framework, and AI virtual assistant provider in the world as measured by market share. On the list of most valuable brands, Google is ranked second by Forbes[18] and fourth by Interbrand.[19] It has received significant criticism involving issues such as privacy concerns, tax avoidance, censorship, search neutrality, antitrust and abuse of its monopoly position.\n",
      "\n",
      "after:\n",
      "Google LLC is an American multinational technology company focusing on search engine technology, online advertising, cloud computing, computer software, quantum computing, e-commerce, artificial intelligence and consumer electronics. It has been referred to as \"the most powerful company in the world and one of the world's most valuable brands due to its market dominance, data collection, and technological advantages in the area of artificial intelligence Its parent company Alphabet is considered one of the Big Five American information technology companies, alongside Amazon, Apple, Meta, and Microsoft.Google was founded on September 4, 1998, by Larry Page and Sergey Brin while they were PhD students at Stanford University in California. Together they own about 14% of its publicly listed shares and control 56% of the stockholder voting power through super-voting stock. The company went public via an initial public offering (IPO) in 2004. In 2015, Google was reorganized as a wholly owned subsidiary of Alphabet Inc. Google is Alphabet's largest subsidiary and is a holding company for Alphabet's Internet properties and interests. Sundar Pichai was appointed CEO of Google on October 24, 2015, replacing Larry Page, who became the CEO of Alphabet. On December 3, 2019, Pichai also became the CEO of AlphabetThe company has since rapidly grown to offer a multitude of products and services beyond Google Search, many of which hold dominant market positions. These products address a wide range of use cases, including email (Gmail), navigation (Waze & Maps), cloud computing (Cloud), web browsing (Chrome), video sharing (YouTube), productivity (Workspace), operating systems (Android), cloud storage (Drive), language translation (Translate), photo storage (Photos), video calling (Meet), smart home (Nest), smartphones (Pixel), wearable technology (Pixel Watch & Fitbit), music streaming (YouTube Music), video on demand (YouTube TV), artificial intelligence (Google Assistant), machine learning APIs (TensorFlow), AI chips (TPU), and more. Discontinued Google products include gaming (Stadia), Glass,[citation needed] Google+, Reader, Play Music, Nexus, Hangouts, and Inbox by GmailGoogle's other ventures outside of Internet services and consumer electronics include quantum computing (Sycamore), self-driving cars (Waymo, formerly the Google Self-Driving Car Project), smart cities (Sidewalk Labs), and transformer models (Google Brain)Google and YouTube are the two most visited websites worldwide followed by Facebook and Twitter. Google is also the largest search engine, mapping and navigation application, email provider, office suite, video sharing platform, photo and cloud storage provider, mobile operating system, web browser, ML framework, and AI virtual assistant provider in the world as measured by market share. On the list of most valuable brands, Google is ranked second by Forbe and fourth by Interbrand It has received significant criticism involving issues such as privacy concerns, tax avoidance, censorship, search neutrality, antitrust and abuse of its monopoly position.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 줄이 바뀌는 텍스트를 인용할 경우 인용부호로 \"\"\"를 사용\n",
    "article = \"\"\"\n",
    "Google LLC is an American multinational technology company focusing on search engine technology, online advertising, cloud computing, computer software, quantum computing, e-commerce, artificial intelligence,[9] and consumer electronics. It has been referred to as \"the most powerful company in the world\"[10] and one of the world's most valuable brands due to its market dominance, data collection, and technological advantages in the area of artificial intelligence.[11][12][13] Its parent company Alphabet is considered one of the Big Five American information technology companies, alongside Amazon, Apple, Meta, and Microsoft.\n",
    "\n",
    "Google was founded on September 4, 1998, by Larry Page and Sergey Brin while they were PhD students at Stanford University in California. Together they own about 14% of its publicly listed shares and control 56% of the stockholder voting power through super-voting stock. The company went public via an initial public offering (IPO) in 2004. In 2015, Google was reorganized as a wholly owned subsidiary of Alphabet Inc. Google is Alphabet's largest subsidiary and is a holding company for Alphabet's Internet properties and interests. Sundar Pichai was appointed CEO of Google on October 24, 2015, replacing Larry Page, who became the CEO of Alphabet. On December 3, 2019, Pichai also became the CEO of Alphabet.[14]\n",
    "\n",
    "The company has since rapidly grown to offer a multitude of products and services beyond Google Search, many of which hold dominant market positions. These products address a wide range of use cases, including email (Gmail), navigation (Waze & Maps), cloud computing (Cloud), web browsing (Chrome), video sharing (YouTube), productivity (Workspace), operating systems (Android), cloud storage (Drive), language translation (Translate), photo storage (Photos), video calling (Meet), smart home (Nest), smartphones (Pixel), wearable technology (Pixel Watch & Fitbit), music streaming (YouTube Music), video on demand (YouTube TV), artificial intelligence (Google Assistant), machine learning APIs (TensorFlow), AI chips (TPU), and more. Discontinued Google products include gaming (Stadia), Glass,[citation needed] Google+, Reader, Play Music, Nexus, Hangouts, and Inbox by Gmail.[15][16]\n",
    "\n",
    "Google's other ventures outside of Internet services and consumer electronics include quantum computing (Sycamore), self-driving cars (Waymo, formerly the Google Self-Driving Car Project), smart cities (Sidewalk Labs), and transformer models (Google Brain).[17]\n",
    "\n",
    "Google and YouTube are the two most visited websites worldwide followed by Facebook and Twitter. Google is also the largest search engine, mapping and navigation application, email provider, office suite, video sharing platform, photo and cloud storage provider, mobile operating system, web browser, ML framework, and AI virtual assistant provider in the world as measured by market share. On the list of most valuable brands, Google is ranked second by Forbes[18] and fourth by Interbrand.[19] It has received significant criticism involving issues such as privacy concerns, tax avoidance, censorship, search neutrality, antitrust and abuse of its monopoly position.\n",
    "\"\"\"\n",
    "\n",
    "print(\"before:\")\n",
    "print(article)\n",
    "\n",
    "# 정규식 적용\n",
    "article = re.sub(r\"[:.]\\[[0-9]+\\](.*?)\\([0-9]+\\)|.?[([][0-9]+[])]|\\n|\\r\", r\"\", article)\n",
    "\n",
    "print(\"after:\")\n",
    "print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 20441,  2291,    16,    41,   470, 17043,   806,   138,  5650,\n",
       "            15,  1707,  3819,   806,     6,   804,  4579,     6,  3613, 11730,\n",
       "             6,  3034,  2257,     6, 17997, 11730,     6,   364,    12,  8342,\n",
       "             6,  7350,  2316,     8,  2267,  8917,     4,    85,    34,    57,\n",
       "          4997,     7,    25,    22,   627,   144,  2247,   138,    11,     5,\n",
       "           232,     8,    65,     9,     5,   232,    18,   144,  5130,  3595,\n",
       "           528,     7,    63,   210, 12532,     6,   414,  2783,     6,     8,\n",
       "          9874, 12340,    11,     5,   443,     9,  7350,  2316,  3139,  4095,\n",
       "           138, 15023,    16,  1687,    65,     9,     5,  1776,  4934,   470,\n",
       "           335,   806,   451,     6,  2863,  1645,     6,  1257,     6, 37622,\n",
       "             6,     8,  3709,     4, 20441,    21,  4790,    15,   772,   204,\n",
       "             6,  6708,     6,    30,  6045,  7086,     8, 23509,  2265,   179,\n",
       "           150,    51,    58, 15221,   521,    23,  8607,   589,    11,   886,\n",
       "             4, 10853,    51,   308,    59,   501,   207,     9,    63,  3271,\n",
       "          3147,   327,     8,   797,  4772,   207,     9,     5,   388, 14074,\n",
       "          3434,   476,   149,  2422,    12,   705, 12653,   388,     4,    20,\n",
       "           138,   439,   285,  1241,    41,  2557,   285,  1839,    36,  3808,\n",
       "           673,    43,    11,  4482,     4,    96,   570,     6,  1204,    21,\n",
       "           769, 29835,    25,    10, 16123,  2164,  8540,     9, 15023,   603,\n",
       "             4,  1204,    16, 15023,    18,  1154,  8540,     8,    16,    10,\n",
       "          1826,   138,    13, 15023,    18,  3742,  3611,     8,  3168,     4,\n",
       "         12282,   271,   221,  1725,  1439,    21,  3873,  1324,     9,  1204,\n",
       "            15,   779,   706,     6,   570,     6,  8119,  6045,  7086,     6,\n",
       "            54,  1059,     5,  1324,     9, 15023,     4,   374,   719,   155,\n",
       "             6,   954,     6,   221,  1725,  1439,    67,  1059,     5,  1324,\n",
       "             9, 15023,   133,   138,    34,   187,  6042,  3831,     7,   904,\n",
       "            10, 25180,     9,   785,     8,   518,  1684,  1204, 12180,     6,\n",
       "           171,     9,    61,   946,  7353,   210,  2452,     4,  1216,   785,\n",
       "          1100,    10,  1810,  1186,     9,   304,  1200,     6,   217,  1047,\n",
       "            36,   534,  6380,   238, 14461,    36,   771, 10129,   359, 21089,\n",
       "           238,  3613, 11730,    36, 14438,   238,  3748, 24033,    36,  4771,\n",
       "         20169,   238,   569,  3565,    36, 36169,   238,  8106,    36, 26025,\n",
       "         18851,   238,  1633,  1743,    36, 42375,   238,  3613,  3521,    36,\n",
       "         29279,   238,  2777, 19850,    36, 19163, 19593,   238,  1345,  3521,\n",
       "            36, 38580,   238,   569,  1765,    36, 30682,   238,  2793,   184,\n",
       "            36,   487,   990,   238,  7466,    36, 46548,   238, 23541,   806,\n",
       "            36, 46548,  3075,   359, 14950,  5881,   238,   930,  5230,    36,\n",
       "         36169,  3920,   238,   569,    15,  1077,    36, 36169,  1012,   238,\n",
       "          7350,  2316,    36, 20441,  6267,   238,  3563,  2239, 35329,    36,\n",
       "           565, 35354, 41779,   238,  4687,  8053,    36,   565, 16821,   238,\n",
       "             8,    55,     4, 19020,  2533,   179,  6796,  1204,   785,   680,\n",
       "          6548,    36,  5320, 20329,   238, 10352, 47789,   438, 12257,   956,\n",
       "           742,  1204, 30787, 27019,     6,  3902,  3920,     6, 28645,     6,\n",
       "         12403,  4518,     6,     8,    96,  8304,    30, 29004, 20441,    18,\n",
       "            97, 16186,   751,     9,  3742,   518,     8,  2267,  8917,   680,\n",
       "         17997, 11730,    36, 35615, 16767,  1688,   238,  1403,    12, 10241,\n",
       "          1677,    36, 24450,  4992,     6,  9598,     5,  1204, 12156,    12,\n",
       "         34002,  6645,  1653,  3728,   238,  2793,  1947,    36,   104,   808,\n",
       "          2753,  9707, 20404,   238,     8, 40878,  3092,    36, 20441, 19743,\n",
       "            43, 20441,     8,  4037,    32,     5,    80,   144,  3790,  7656,\n",
       "          3612,  1432,    30,   622,     8,   599,     4,  1204,    16,    67,\n",
       "             5,  1154,  1707,  3819,     6, 20410,     8, 14461,  2502,     6,\n",
       "          1047,  3696,     6,   558, 10606,     6,   569,  3565,  1761,     6,\n",
       "          1345,     8,  3613,  3521,  3696,     6,  1830,  1633,   467,     6,\n",
       "          3748, 11407,     6, 10725,  7208,     6,     8,  4687,  6229,  3167,\n",
       "          3696,    11,     5,   232,    25,  9550,    30,   210,   458,     4,\n",
       "           374,     5,   889,     9,   144,  5130,  3595,     6,  1204,    16,\n",
       "          4173,   200,    30,   286,  1610,     8,   887,    30,  3870, 11638,\n",
       "            85,    34,   829,  1233,  3633,  3329,   743,   215,    25,  4144,\n",
       "          1379,     6,   629, 29751,     6, 23915,     6,  1707, 18755,     6,\n",
       "         18849,     8,  2134,     9,    63, 21445,   737,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저로 인코딩\n",
    "inputs = tokenizer([article], max_length=1024, return_tensors='pt', truncation=True)\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 20441,  2291,    16,    41,   470, 17043,   806,   138,\n",
       "         5650,    15,  1707,  3819,   806,     6,   804,  4579,     6,\n",
       "         3613, 11730,     6,  3034,  2257,     6, 17997, 11730,     6,\n",
       "          364,    12,  8342,     6,  7350,  2316,     8,  2267,  8917,\n",
       "            4,    85,    34,    57,  4997,     7,    25,    22,   627,\n",
       "          144,  2247,   138,    11,     5,   232,     8,    65,     9,\n",
       "            5,   232,    18,   144,  5130,  3595,   528,     7,    63,\n",
       "          210, 12532,     6,   414,  2783,     6,     8,  9874, 12340,\n",
       "           11,     5,   443,     9,  7350,  2316,  3139,  4095,   138,\n",
       "        15023,    16,  1687,    65,     9,     5,  1776,  4934,   470,\n",
       "          335,   806,   451,     6,  2863,  1645,     6,  1257,     6,\n",
       "        37622,     6,     8,  3709,     4, 20441,    21,  4790,    15,\n",
       "          772,   204,     6,  6708,     6,    30,  6045,  7086,     8,\n",
       "        23509,  2265,   179,   150,    51,    58, 15221,   521,    23,\n",
       "         8607,   589,    11,   886,     4, 10853,    51,   308,    59,\n",
       "          501,   207,     9,    63,  3271,  3147,   327,     8,   797,\n",
       "         4772,   207,     9,     5,   388, 14074,  3434,   476,   149,\n",
       "         2422,    12,   705, 12653,   388,     4,    20,   138,   439,\n",
       "          285,  1241,    41,  2557,   285,  1839,    36,  3808,   673,\n",
       "           43,    11,  4482,     4,    96,   570,     6,  1204,    21,\n",
       "          769, 29835,    25,    10, 16123,  2164,  8540,     9, 15023,\n",
       "          603,     4,  1204,    16, 15023,    18,  1154,  8540,     8,\n",
       "           16,    10,  1826,   138,    13, 15023,    18,  3742,  3611,\n",
       "            8,  3168,     4, 12282,   271,   221,  1725,  1439,    21,\n",
       "         3873,  1324,     9,  1204,    15,   779,   706,     6,   570,\n",
       "            6,  8119,  6045,  7086,     6,    54,  1059,     5,  1324,\n",
       "            9, 15023,     4,   374,   719,   155,     6,   954,     6,\n",
       "          221,  1725,  1439,    67,  1059,     5,  1324,     9, 15023,\n",
       "          133,   138,    34,   187,  6042,  3831,     7,   904,    10,\n",
       "        25180,     9,   785,     8,   518,  1684,  1204, 12180,     6,\n",
       "          171,     9,    61,   946,  7353,   210,  2452,     4,  1216,\n",
       "          785,  1100,    10,  1810,  1186,     9,   304,  1200,     6,\n",
       "          217,  1047,    36,   534,  6380,   238, 14461,    36,   771,\n",
       "        10129,   359, 21089,   238,  3613, 11730,    36, 14438,   238,\n",
       "         3748, 24033,    36,  4771, 20169,   238,   569,  3565,    36,\n",
       "        36169,   238,  8106,    36, 26025, 18851,   238,  1633,  1743,\n",
       "           36, 42375,   238,  3613,  3521,    36, 29279,   238,  2777,\n",
       "        19850,    36, 19163, 19593,   238,  1345,  3521,    36, 38580,\n",
       "          238,   569,  1765,    36, 30682,   238,  2793,   184,    36,\n",
       "          487,   990,   238,  7466,    36, 46548,   238, 23541,   806,\n",
       "           36, 46548,  3075,   359, 14950,  5881,   238,   930,  5230,\n",
       "           36, 36169,  3920,   238,   569,    15,  1077,    36, 36169,\n",
       "         1012,   238,  7350,  2316,    36, 20441,  6267,   238,  3563,\n",
       "         2239, 35329,    36,   565, 35354, 41779,   238,  4687,  8053,\n",
       "           36,   565, 16821,   238,     8,    55,     4, 19020,  2533,\n",
       "          179,  6796,  1204,   785,   680,  6548,    36,  5320, 20329,\n",
       "          238, 10352, 47789,   438, 12257,   956,   742,  1204, 30787,\n",
       "        27019,     6,  3902,  3920,     6, 28645,     6, 12403,  4518,\n",
       "            6,     8,    96,  8304,    30, 29004, 20441,    18,    97,\n",
       "        16186,   751,     9,  3742,   518,     8,  2267,  8917,   680,\n",
       "        17997, 11730,    36, 35615, 16767,  1688,   238,  1403,    12,\n",
       "        10241,  1677,    36, 24450,  4992,     6,  9598,     5,  1204,\n",
       "        12156,    12, 34002,  6645,  1653,  3728,   238,  2793,  1947,\n",
       "           36,   104,   808,  2753,  9707, 20404,   238,     8, 40878,\n",
       "         3092,    36, 20441, 19743,    43, 20441,     8,  4037,    32,\n",
       "            5,    80,   144,  3790,  7656,  3612,  1432,    30,   622,\n",
       "            8,   599,     4,  1204,    16,    67,     5,  1154,  1707,\n",
       "         3819,     6, 20410,     8, 14461,  2502,     6,  1047,  3696,\n",
       "            6,   558, 10606,     6,   569,  3565,  1761,     6,  1345,\n",
       "            8,  3613,  3521,  3696,     6,  1830,  1633,   467,     6,\n",
       "         3748, 11407,     6, 10725,  7208,     6,     8,  4687,  6229,\n",
       "         3167,  3696,    11,     5,   232,    25,  9550,    30,   210,\n",
       "          458,     4,   374,     5,   889,     9,   144,  5130,  3595,\n",
       "            6,  1204,    16,  4173,   200,    30,   286,  1610,     8,\n",
       "          887,    30,  3870, 11638,    85,    34,   829,  1233,  3633,\n",
       "         3329,   743,   215,    25,  4144,  1379,     6,   629, 29751,\n",
       "            6, 23915,     6,  1707, 18755,     6, 18849,     8,  2134,\n",
       "            9,    63, 21445,   737,     4,     2]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텐서플로우 타입인 inputs['input_ids]를 넘파이 어레이 타입으로 변경\n",
    "inputs['input_ids'].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors\n",
      "tensor([[    2,     0, 20441,  2291,    16,    41,   470, 17043,   806,   138,\n",
      "          5650,    15,  1707,  3819,   806,     6,   804,  4579,     6,  3613,\n",
      "         11730,     6,  3034,  2257,     2]])\n",
      "decoded\n",
      "Google LLC is an American multinational technology company focusing on search engine technology, online advertising, cloud computing, computer software\n"
     ]
    }
   ],
   "source": [
    "# 런타임 30초 소요\n",
    "# 문장 요약 생성 결과(vocabulary ID)를 summary_ids에 저장\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=5, max_length=25)\n",
    "print(\"Vectors\")\n",
    "print(summary_ids)\n",
    "# summary_ids 디코딩\n",
    "print(\"decoded\")\n",
    "print(''.join([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [[\"What music do you like?\", \"I like Rock music.\", 1],\n",
    "           [\"What is your favorite food?\", \"I like sushi the best\", 1],\n",
    "           [\"What is your favorite color?\", \"I'm going to be a doctor\", 0],\n",
    "           [\"What is your favorite song?\", \"Tokyo olympic game in 2020 was postponed\", 0],\n",
    "           [\"Do you like watching TV shows?\", \"Yeah, I often watch it in my spare time\", 1]]\n",
    "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n",
    "from torch import nn\n",
    "\n",
    "# 클래스 정의\n",
    "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
    "  def __init__(self, config, *args, **kwargs):\n",
    "      super().__init__(config)\n",
    "\n",
    "      # QA BERT 모델\n",
    "      self.bert_model_1 = BertModel(config)\n",
    "      # AQ BERT 모델\n",
    "      self.bert_model_2 = BertModel(config)\n",
    "\n",
    "      # 선형함수\n",
    "      self.cls = nn.Linear(2 * self.config.hidden_size, 2)\n",
    "      # 초기 가중치\n",
    "      self.init_weights()\n",
    "\n",
    "  def forward(\n",
    "          self,\n",
    "          input_ids=None,\n",
    "          attention_mask=None,\n",
    "          token_type_ids=None,\n",
    "          position_ids=None,\n",
    "          head_mask=None,\n",
    "          inputs_embeds=None,\n",
    "          next_sentence_label=None,\n",
    "  ):\n",
    "    # 빈 컨테이너 변수 outputs 생성\n",
    "    outputs = []\n",
    "\n",
    "    # input_ids 첫번째 입력(문장) 저장\n",
    "    input_ids_1 = input_ids[0]\n",
    "\n",
    "    # input_ids 첫번째 입력(문장)의 attention_mask 저장\n",
    "    attention_mask_1 = attention_mask[0]\n",
    "\n",
    "    # bert_model_1에 input_ids_1 투입한 결과를 outputs에 순차적으로 저장\n",
    "    outputs.append(self.bert_model_1(input_ids_1,\n",
    "                                     attention_mask=attention_mask_1))\n",
    "\n",
    "    # input_ids 두번째 입력(문장) 저장\n",
    "    input_ids_2 = input_ids[1]\n",
    "\n",
    "    # input_ids 두번째 입력(문장)의 attention_mask 저장\n",
    "    attention_mask_2 = attention_mask[1]\n",
    "\n",
    "    # bert_model_2에 input_ids_2 투입한 결과를 outputs에 순차적으로 저장\n",
    "    outputs.append(self.bert_model_2(input_ids_2,\n",
    "                                     attention_mask=attention_mask_2))\n",
    "\n",
    "    # outputs에 쌓인 output의 두번째 요소(output[1])을 하나하나 끄집어내어\n",
    "    # torch.cat()로 토치 텐서 형태로 병합\n",
    "    # 이를 통해 마지막 은닉층 임베딩 상태를 구함\n",
    "    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
    "    # self.cls 선형함수에 마지막 은닉층 임베딩 상태를 투입하여 로짓 추출\n",
    "    logits = self.cls(last_hidden_states)\n",
    "\n",
    "    # 크로스엔트로피 손실(crossentropyloss) 구하기\n",
    "    # crossentropyloss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "    if next_sentence_label is not None:\n",
    "      # nn.CrossEntropyLoss( ) 입력 데이터의 마지막 인덱스는 계산에서 제외\n",
    "      loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "      # logits.view(-1,2)는 열이 두 개 형태로 logits를 정렬\n",
    "      # next_sentence_label.view(-1)는 행이 하나인 형태(flattening)로 정렬\n",
    "      next_sentence_loss = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n",
    "      return next_sentence_loss, logits\n",
    "    else:\n",
    "      return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 9, 4],\n",
      "        [8, 6, 6]])\n",
      "tensor([[2],\n",
      "        [9],\n",
      "        [4],\n",
      "        [8],\n",
      "        [6],\n",
      "        [6]])\n",
      "tensor([2, 9, 4, 8, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0,10,(2,3))\n",
    "print(x)\n",
    "print(x.view(6,-1))\n",
    "print(x.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\transformer\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "# 코랩에서 가능한 경우 GPU를 사용하고, 그렇지 않으면 CPU 사용하도록\n",
    "# 변수 device로 설정을 저장\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 및 config 설정\n",
    "config = BertConfig()\n",
    "model = BertEnsembleForNextSentencePrediction(config)\n",
    "\n",
    "# 모델을 변수 device에서 설정한 대로 GPU 혹은 CPU로 전송\n",
    "model.to(device)\n",
    "\n",
    "# 토크니이저 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 학습률 설정\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# 절편과 가중치를 no_decay 변수에 저장\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "# 최적화 함수 그룹 파라미터 설정\n",
    "optimizer_grouped_parameters = [{\n",
    "  \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "  }]\n",
    "\n",
    "# 최적화 함수 설정\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#함수 prepare_data 정의\n",
    "def prepare_data(dataset, qa=True):\n",
    "\n",
    "  # 빈 컨테이너 변수 생성\n",
    "  input_ids, attention_masks = [], []\n",
    "  labels = []\n",
    "\n",
    "  for point in dataset:\n",
    "    if qa is True:\n",
    "      # point에 있는 3개의 원소를 앞에 요소부터 q, a, _ 으로 배정\n",
    "      q, a, _ = point\n",
    "    else:\n",
    "      # point에 있는 3개의 원소를 앞에 요소부터 a, q, _ 으로 배정\n",
    "      a, q, _ = point\n",
    "    # q와 a를 토크나이저를 통해 인코딩\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "      q,  # 문장 1 인코딩\n",
    "      a,  # 문장 2 인코딩\n",
    "      add_special_tokens=True,  # 특수 토큰인 [CLS]와 [SEP] 생성\n",
    "      max_length=128,           # max_length 설정Pad & truncate all sentences.\n",
    "      pad_to_max_length=True,   # max_length까지 패딩 실행\n",
    "      return_attention_mask=True,  # attention_mask 생성\n",
    "      return_tensors='pt',      # 파이토치 텐서 타입으로 출력\n",
    "      truncation=True           # truncation 실행\n",
    "    )\n",
    "\n",
    "    # encoded_dict(\"input_ids\")를 컨테이너 변수 input_ids에 순서대로 저장\n",
    "    input_ids.append(encoded_dict[\"input_ids\"])\n",
    "    # encoded_dict(\"attention_mask\")를 컨테이너 변수 attention_mask에 순서대로 저장\n",
    "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "    # point의 마지막(세번째) 요소(레이블)을 컨테이너 변수 labels에 순서대로 저장\n",
    "    labels.append(point[-1])\n",
    "    # 반복문 종료\n",
    "\n",
    "  # input_ids를 첫번째 축(dim=0), 즉 세로 방향으로 병합\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "\n",
    "  # attention_mask를 첫번째 축(dim=0), 즉 세로 방향으로 병합\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "  # 함수의 결과물로 input_ids, attention_mask, lables 출력\n",
    "  return input_ids, attention_masks, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
    "\n",
    "# QADataset 클래스 생성\n",
    "class QADataset(Dataset):\n",
    "\n",
    "  # input_ids 텐서와 attention_masks 텐서 생성\n",
    "  def __init__(self, input_ids, attention_masks, labels=None):\n",
    "    self.input_ids = np.array(input_ids)\n",
    "    self.attention_masks = np.array(attention_masks)\n",
    "    # toch.long은 정수(integer) 타입을 의미\n",
    "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.input_ids[index], self.attention_masks[index], self.labels[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.input_ids.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\transformer\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# dataset을 prepare_data 함수에 투입하여 그 결과를 각기\n",
    "# input_ids_qa, attention_maskes_qa, labels_qa에 저장\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
    "\n",
    "# 위의 세 결과물을 QADataset 클래스에 투입\n",
    "train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "# 맨 윗줄 코드와 동일하나 이번에는 prepare_data 함수에 qa 플래그 값이 False일 때를 적용\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
    "\n",
    "# 바로 위 세 결과물을 QADataset 클래스에 투입\n",
    "train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "# train_dataset_qa를 DataLoader로 처리\n",
    "dataloader_qa =  DataLoader(dataset=train_dataset_qa,\n",
    "                            batch_size=5,\n",
    "                            sampler=SequentialSampler(train_dataset_qa))\n",
    "# train_dataset_aq를 DataLoader로 처리\n",
    "dataloader_aq =  DataLoader(dataset=train_dataset_aq,\n",
    "                            batch_size=5,\n",
    "                            sampler=SequentialSampler(train_dataset_aq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.7840352058410645\n",
      "epoch:1, loss:0.7113007307052612\n",
      "epoch:2, loss:0.6796483397483826\n",
      "epoch:3, loss:0.5578693151473999\n",
      "epoch:4, loss:0.5462298393249512\n",
      "epoch:5, loss:0.49122634530067444\n",
      "epoch:6, loss:0.46497970819473267\n",
      "epoch:7, loss:0.5190169215202332\n",
      "epoch:8, loss:0.42059651017189026\n",
      "epoch:9, loss:0.3347664773464203\n",
      "epoch:10, loss:0.4123448431491852\n",
      "epoch:11, loss:0.27292197942733765\n",
      "epoch:12, loss:0.293318510055542\n",
      "epoch:13, loss:0.23840391635894775\n",
      "epoch:14, loss:0.2577614188194275\n",
      "epoch:15, loss:0.20953114330768585\n",
      "epoch:16, loss:0.18924881517887115\n",
      "epoch:17, loss:0.1678706705570221\n",
      "epoch:18, loss:0.12091512978076935\n",
      "epoch:19, loss:0.08506893366575241\n",
      "epoch:20, loss:0.0719650462269783\n",
      "epoch:21, loss:0.05616874620318413\n",
      "epoch:22, loss:0.056133974343538284\n",
      "epoch:23, loss:0.027188073843717575\n",
      "epoch:24, loss:0.035054296255111694\n",
      "epoch:25, loss:0.01944793201982975\n",
      "epoch:26, loss:0.01767650805413723\n",
      "epoch:27, loss:0.02308240719139576\n",
      "epoch:28, loss:0.013324430212378502\n",
      "epoch:29, loss:0.010152528062462807\n"
     ]
    }
   ],
   "source": [
    "# 런타임 20, 30초 이내, 단 GPU 미사용시 런타임 약 8분 소요\n",
    "# 에포크 횟수 지정\n",
    "epochs = 30\n",
    "\n",
    "# 에포크 횟수만큼 반복 루프 실행\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  # dataloader_qa와 datalaode_aq 쌍(pair)를 동시에 반복 루프에서 처리\n",
    "  # enumerate 및 zip으로 두 데이터 쌍을 묶고 반복가능한 순서 부여\n",
    "  # zip은 반복가능한 객체를 받아 원소를 튜플로 변환\n",
    "  for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "    # enumerate로 묶은 두 데이터쌍을 순서대로 batch_1과 batch_2에 저장\n",
    "    batch_1, batch_2 = combined_batch\n",
    "    #모델을 학습 모드로 전환\n",
    "    model.train()\n",
    "\n",
    "    # 가능한 경우 batch_1과 batch_2의 데이터를 GPU로 전달하고\n",
    "    # 그렇지 않은 경우 CPU로 전달\n",
    "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "    # 모델에 투입할 변수 inputs의 내용 입력\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "        \"next_sentence_label\": batch_1[2]\n",
    "    }\n",
    "    # 모델에 inputs를 **kwargs 형식(**inputs로 표기)으로 투입\n",
    "    # 즉, 딕셔너리 타입인 inputs의 키(key)와 키값(value) 모두 입력\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # 모델의 결과물인 outputs는 튜플 타입으로 출력\n",
    "    # 그중 첫번째 요소, 즉 outputs[0]을 변수 loss에 저장\n",
    "    loss = outputs[0]\n",
    "    # 오차 역전파\n",
    "    loss.backward()\n",
    "    # f 문자열을 사용하여 에포크와 손실 출력\n",
    "    print(f\"epoch:{epoch}, loss:{loss}\")\n",
    "\n",
    "    # 가중치 업데이트\n",
    "    optimizer.step()\n",
    "    # 다음 번 에포크를 위해 그래디언트(기울기) 초기화\n",
    "    model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 1] [1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# dataset을 prepare_data 함수로 처리\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
    "# 위 결과물을 QADataset 클래스로 처리\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "# dataset을 prepare_data 함수로 처리하되 qa 플래그를 False로 적용\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
    "# 위 결과물을 QADataset 클래스로 처리\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "# test_dataset_qa를 DataLoader로 처리\n",
    "dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_qa))\n",
    "# test_dataset_aq를 DataLoader로 처리\n",
    "dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_aq))\n",
    "\n",
    "# 결과를 담을 빈 컨테이너로서 리스트 변수 생성\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "# dataloader_qa와 dataloade_aq를 동시에 반복 루프 작업 실시\n",
    "\n",
    "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "\n",
    "  # 모델을 eval 모드로 전환\n",
    "  model.eval()\n",
    "  # enumerate로 묶은 두 데이터 쌍을 순서대로 batch_1과 batch_2에 저장\n",
    "  batch_1, batch_2 = combined_batch\n",
    "\n",
    "  # 가능한 경우 batch_1과 batch2의 데이터를 GPU로 전달하고\n",
    "  # 그렇지 않은 경우 CPU로 전달\n",
    "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "  # 추론(evaluation) 때는 순전파(forward propagation) 과정만 필요하기에\n",
    "  # 그래디언트(기울기) 계산이 필요 없음. 아래 코딩 줄은 자동미분을 방지.\n",
    "  with torch.no_grad():\n",
    "\n",
    "    # 모델에 투입할 변수 inputs의 내용 입력\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "        \"next_sentence_label\": batch_1[2]\n",
    "    }\n",
    "\n",
    "    # 모델에 inputs를 **kwargs 형식(**inputs로 표기)으로 투입\n",
    "    # 즉, 딕셔너리 타입인 inputs의 키(key)와 키값(value) 모두 입력\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # outputs의 첫번째 요소를 tmp_eval_loss에 저장\n",
    "    # outputs의 두번째 요소를 logits에 저장\n",
    "    tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "    # 로짓을 CPU로 전달하고 넘파이 값으로 변환\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # logits에 담긴 로짓값을 축(axis) 1, 즉 가로방향으로 최대값을 갖는 인덱스 추출\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    # inputs['next_sentence_label']을 CPU로 전달하고 넘파이 값으로 변환\n",
    "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
    "    # with torch.no_grad(): 구문 종료\n",
    "\n",
    "  # outputs과 label_ids를 각각의 컨테이너 리스트 변수에 순서대로 저장\n",
    "  complete_outputs.extend(outputs)\n",
    "  complete_label_ids.extend(label_ids)\n",
    "\n",
    "# 최종 결과물 출력\n",
    "print(complete_outputs, complete_label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [1]\n"
     ]
    }
   ],
   "source": [
    "# 이 코드의 설명은 앞의 문제들 주석과 동일\n",
    "dataset = [[\"What music do you like?\", \"I like Rock music.\", 1]]\n",
    "\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_qa))\n",
    "dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n",
    "                            batch_size=16,\n",
    "                            sampler=SequentialSampler(test_dataset_aq))\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "  model.eval()\n",
    "  batch_1, batch_2 = combined_batch\n",
    "\n",
    "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "        \"next_sentence_label\": batch_1[2]\n",
    "    }\n",
    "    outputs = model(**inputs)\n",
    "    tmp_eval_loss, logits = outputs[:2]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
    "  complete_outputs.extend(outputs)\n",
    "  complete_label_ids.extend(label_ids)\n",
    "\n",
    "print(complete_outputs, complete_label_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
